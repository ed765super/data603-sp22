{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DATA603 hw07.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This FINAL homework recaps the Spark ML library:\n",
        "\n",
        "0) Download the \"Rain in Australia\" dataset from Kaggle (it is also attached to this assigbnment): https://www.kaggle.com/jsphyg/weather-dataset-rattle-package\n",
        "\n",
        "1) In a Jupyter Notebook, write a SparkML script that uses a Decision Tree Classifier to predict the RainTomorrow target varible\n",
        "\n",
        "2) Split the data 80/20 train/test, using a seed of 12345\n",
        "\n",
        "3) Use transformers to remove unnecessary columns (use your best judgement) and convert categorical variables into one-hot encoded variables\n",
        "\n",
        "4) Use a parameter grid to determine the best parameters for:\n",
        "impurity - gini, entropy\n",
        "maxBins - 5, 10, 15\n",
        "minInfoGain - 0.0, 0.2, 0.4\n",
        "maxDepth - 3, 5, 7\n",
        "\n",
        "5) Cross-validate with 4 folds\n",
        "\n",
        "6) Use a pipeline to encapsulate all steps\n",
        "\n",
        "7) Print the parameters from the best model selected\n",
        "\n",
        "8) Calculate and print the Area under ROC Curve and Area under Precision-Recall Curve scores for your training and test data sets (these are built-in metrics, you do not need to calculate anything by hand)\n",
        "\n",
        "Your script should be clean of all the testing and exploration and should only contain the necessary code to satisfy the above conditions"
      ],
      "metadata": {
        "id": "KFUesUIE-Zk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtCK7jDdA3NI",
        "outputId": "777c7056-5d20-48b2-d078-c2b6d44705a7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 35 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 51.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=aa98aaa8287ca5dadb48ea98feed2099d6441ace13d1863c4b4c58b5b1b610e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "#I cnonstantly forget to do this part\n",
        "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "XRa7AfjUCACh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "_z9bdWNY9wyI"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.feature import StringIndexer, VectorIndexer, OneHotEncoder\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.types import *\n",
        "import pandas as pd\n",
        "\n",
        "mySchema = StructType([\n",
        "      StructField(\"Date\",StringType(),True),\n",
        "      StructField(\"Location\",StringType(),True),\n",
        "      StructField(\"MinTemp\",FloatType(),True),\n",
        "      StructField(\"MaxTemp\",FloatType(),True),\n",
        "      StructField(\"Rainfall\",FloatType(),True),\n",
        "      StructField(\"Evaporation\",FloatType(),True),\n",
        "      StructField(\"Sunshine\",FloatType(),True),\n",
        "      StructField(\"WindGustDir\",StringType(),True),\n",
        "      StructField(\"WindGustSpeed\",IntegerType(),True),\n",
        "      StructField(\"WindDir9am\",StringType(),True),\n",
        "      StructField(\"WindDir3pm\",StringType(),True),\n",
        "      StructField(\"WindSpeed9am\",IntegerType(),True),\n",
        "      StructField(\"WindSpeed3pm\",IntegerType(),True),\n",
        "      StructField(\"Humidity9am\",IntegerType(),True),\n",
        "      StructField(\"Humidity3pm\",IntegerType(),True),\n",
        "      StructField(\"Pressure9am\",FloatType(),True),\n",
        "      StructField(\"Pressure3pm\",FloatType(),True),\n",
        "      StructField(\"Cloud9am\",IntegerType(),True),\n",
        "      StructField(\"Cloud3pm\",IntegerType(),True),\n",
        "      StructField(\"Temp9am\",FloatType(),True),\n",
        "      StructField(\"Temp3pm\",FloatType(),True),\n",
        "      StructField(\"RainToday\",StringType(),True),\n",
        "      StructField(\"RainTomorrow\",StringType(),True)\n",
        "])\n",
        "data = spark.read.csv(\"weatherAUS.csv\", header=True, nullValue= 'NA', schema=mySchema)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bazxTT8MCr26",
        "outputId": "c826e494-fa2f-4675-a4b2-aca718d43aed"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+------------+\n",
            "|      Date|Location|MinTemp|MaxTemp|Rainfall|Evaporation|Sunshine|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|Cloud9am|Cloud3pm|Temp9am|Temp3pm|RainToday|RainTomorrow|\n",
            "+----------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+------------+\n",
            "|2008-12-01|  Albury|   13.4|   22.9|     0.6|       null|    null|          W|           44|         W|       WNW|          20|          24|         71|         22|     1007.7|     1007.1|       8|    null|   16.9|   21.8|       No|          No|\n",
            "|2008-12-02|  Albury|    7.4|   25.1|     0.0|       null|    null|        WNW|           44|       NNW|       WSW|           4|          22|         44|         25|     1010.6|     1007.8|    null|    null|   17.2|   24.3|       No|          No|\n",
            "|2008-12-03|  Albury|   12.9|   25.7|     0.0|       null|    null|        WSW|           46|         W|       WSW|          19|          26|         38|         30|     1007.6|     1008.7|    null|       2|   21.0|   23.2|       No|          No|\n",
            "|2008-12-04|  Albury|    9.2|   28.0|     0.0|       null|    null|         NE|           24|        SE|         E|          11|           9|         45|         16|     1017.6|     1012.8|    null|    null|   18.1|   26.5|       No|          No|\n",
            "|2008-12-05|  Albury|   17.5|   32.3|     1.0|       null|    null|          W|           41|       ENE|        NW|           7|          20|         82|         33|     1010.8|     1006.0|       7|       8|   17.8|   29.7|       No|          No|\n",
            "|2008-12-06|  Albury|   14.6|   29.7|     0.2|       null|    null|        WNW|           56|         W|         W|          19|          24|         55|         23|     1009.2|     1005.4|    null|    null|   20.6|   28.9|       No|          No|\n",
            "|2008-12-07|  Albury|   14.3|   25.0|     0.0|       null|    null|          W|           50|        SW|         W|          20|          24|         49|         19|     1009.6|     1008.2|       1|    null|   18.1|   24.6|       No|          No|\n",
            "|2008-12-08|  Albury|    7.7|   26.7|     0.0|       null|    null|          W|           35|       SSE|         W|           6|          17|         48|         19|     1013.4|     1010.1|    null|    null|   16.3|   25.5|       No|          No|\n",
            "|2008-12-09|  Albury|    9.7|   31.9|     0.0|       null|    null|        NNW|           80|        SE|        NW|           7|          28|         42|          9|     1008.9|     1003.6|    null|    null|   18.3|   30.2|       No|         Yes|\n",
            "|2008-12-10|  Albury|   13.1|   30.1|     1.4|       null|    null|          W|           28|         S|       SSE|          15|          11|         58|         27|     1007.0|     1005.7|    null|    null|   20.1|   28.2|      Yes|          No|\n",
            "+----------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgpeNrYtMimP",
        "outputId": "a090da91-9ec2-4620-e5f4-e9a172645b5f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- MinTemp: float (nullable = true)\n",
            " |-- MaxTemp: float (nullable = true)\n",
            " |-- Rainfall: float (nullable = true)\n",
            " |-- Evaporation: float (nullable = true)\n",
            " |-- Sunshine: float (nullable = true)\n",
            " |-- WindGustDir: string (nullable = true)\n",
            " |-- WindGustSpeed: integer (nullable = true)\n",
            " |-- WindDir9am: string (nullable = true)\n",
            " |-- WindDir3pm: string (nullable = true)\n",
            " |-- WindSpeed9am: integer (nullable = true)\n",
            " |-- WindSpeed3pm: integer (nullable = true)\n",
            " |-- Humidity9am: integer (nullable = true)\n",
            " |-- Humidity3pm: integer (nullable = true)\n",
            " |-- Pressure9am: float (nullable = true)\n",
            " |-- Pressure3pm: float (nullable = true)\n",
            " |-- Cloud9am: integer (nullable = true)\n",
            " |-- Cloud3pm: integer (nullable = true)\n",
            " |-- Temp9am: float (nullable = true)\n",
            " |-- Temp3pm: float (nullable = true)\n",
            " |-- RainToday: string (nullable = true)\n",
            " |-- RainTomorrow: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sites: \n",
        "\n",
        "Lecture slides:\n",
        "https://docs.google.com/presentation/d/1rM7dnkVBq11PivNYVVFhDjCswybuey2dh47l-K9JUqI/edit#slide=id.p13\n",
        "\n",
        "That one ML descision tree:\n",
        "https://spark.apache.org/docs/1.5.2/ml-decision-tree.html\n",
        "\n",
        "VectorIndexer vs OHE\n",
        "https://stackoverflow.com/questions/63881128/vectorindexer-or-onehotencoder-for-categorical-variables\n",
        "\n",
        "Indexor overflow: \n",
        "https://stackoverflow.com/questions/56585434/pyspark-pipeline-error-when-using-indexer-and-encoder\n",
        "\n",
        "data in an excell spreadsheet: \n",
        "https://docs.google.com/spreadsheets/d/1P49rE8ZJXb3n4KBv6p5Sm-MPdN5JTJcNfWh5YiXky2o/edit#gid=1304608677\n",
        "\n",
        "Remember how to read your schema: \n",
        "https://predictivehacks.com/?all-tips=define-schema-and-load-data-in-pyspark\n",
        "\n",
        "Index mult columns:\n",
        "https://stackoverflow.com/questions/36942233/apply-stringindexer-to-several-columns-in-a-pyspark-dataframe"
      ],
      "metadata": {
        "id": "TvPwkxyRSKzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dfColumns = [\"Date\", \"Location\", \"MinTemp\", \"MaxTemp\", \"Rainfall\", \"Evaporation\",\n",
        "            \"Sunshine\", \"WindGustDir\", \"WindGustSpeed\", \"WindDir9am\", \"WindDir3pm\",\n",
        "            \"WindSpeed9am\", \"WindSpeed3pm\", \"Humidity9am\", \"Humidity3pm\", \n",
        "            \"Pressure9am\", \"Pressure3pm\", \"Cloud9am\", \"Cloud3pm\", \"Temp9am\",\n",
        "            \"Temp3pm\", \"RainToday\"]"
      ],
      "metadata": {
        "id": "x5E7BdXqMNLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "labelIndexer = StringIndexer(inputCol=\"RainTomorrow\", outputCol=\"RainLabel\").fit(data)\n",
        "# Automatically identify categorical features, and index them.\n",
        "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
        "\n",
        "categorical_cols = [\"Location\",\"WindGustDir\", \"WindDir9am\", \"WindDir3pm\", \"RainToday\"] #excluding target column RainTomorrow and Date\n",
        "numerical_cols = [\"MinTemp\", \"MaxTemp\", \"Rainfall\", \"Evaporation\", \"Sunshine\", \"WindGustSpeed\", \"WindSpeed9am\", \"WindSpeed3pm\", \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\", \"Pressure3pm\", \"Cloud9am\", \"Cloud3pm\", \"Temp9am\", \"Temp3pm\"]\n",
        "\n",
        "#dfColumnsVector = np.array(numerical_cols)\n",
        "#\n",
        "featureIndexer = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(data) for column in list(set(data.columns[:-1])-set(['date'])) ]\n",
        "\n",
        "encoder = OneHotEncoder(inputCols= categorical_cols,\n",
        "                        outputCols=[cols + \"_classVec\" for cols in categorical_cols])\n",
        "\n",
        "featureAssembler = VectorAssembler(inputCols=[cols + \"_classVec\" for cols in categorical_cols] + numerical_cols, outputCol=\"assembledFeatures\")\n",
        "\n",
        "(trainingData, testData) = data.randomSplit([0.8, 0.2], seed=12345)\n",
        "dt = DecisionTreeClassifier(labelCol=\"RainLabel\",\n",
        "featuresCol=\"assembledFeatures\")\n",
        "pipeline = Pipeline(stages=[labelIndexer, encoder, featureAssembler, dt])\n",
        "model = pipeline.fit(trainingData)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "JI9UDnw8_dzi",
        "outputId": "da1375da-d65f-4e6f-c760-904c319c4cd5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-4ca7eb0d1dc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m featuresCol=\"assembledFeatures\")\n\u001b[1;32m     21\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabelIndexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureAssembler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Column Location must be of type numeric but was actually of type string."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "H3d4__LzEHd-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}